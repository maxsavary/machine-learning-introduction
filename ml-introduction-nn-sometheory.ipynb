{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network somme theory\n",
    "\n",
    "*Author : maxime.savary@orange.com (v1.0 December 2020)*\n",
    "\n",
    "\n",
    "We introduce in this section the neural networks attached to the field of **Deep Learning**. The key point is the presentation of the **backpropagation** algorithm and the corresponding derivative calculations.\n",
    "\n",
    "An effort has been made to obtain the clearest mathematical formulation, particularly in terms of indices. A more \"packaged\" mode from an algorithmic perspective is proposed in the two references below\n",
    "\n",
    "Andrew NG courses https://www.youtube.com/playlist?list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN (8.1 and following)\n",
    "\n",
    "The complement can be found in the reference cited in the bibliography « Artificial Intelligence : A Modern Approach » and available here http://aima.cs.berkeley.edu/\n",
    "\n",
    "\n",
    "## Neuron Model\n",
    "\n",
    "The objective is to build a classifying system from training data\n",
    "\n",
    "An artificial neuron is a basic unit of a network which receives as input numbers $\\{x_{1}, ..., x_{N}\\}$ which are each multiplied by \"weights\" $\\{w_1, ..., w_N\\}$. If a neuron has $N$ Inputs, it has $N$ weights. These weighted data are added together to produce a logical unit. We add the **bias** which has the value $+1$ and its own weight $b$ then we calculate the output value $h_{\\mathbf{w}}(x)$. This value is then compared to the expected value, here $y =\\{0; 1\\}$\n",
    "\n",
    "![image](./images/deep-ML-Note-01b.png)\n",
    "\n",
    "\n",
    "$$h_{\\mathbf{w}}(x)=\\frac{1}{1+e^{-\\mathbf{w}^{T}.\\mathbf{x}}} \\qquad \\qquad (1.1)$$\n",
    "\n",
    "$h$ is broken down into :\n",
    "* $z = b + \\sum_{k=1}^{N}w_kx_{k}$ \n",
    "* $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "\n",
    "The **bias** $b$ : \n",
    "* The bias acts on the neuron activation level. If we have $z = -20 + w_1x_1 + w_2x_2 + w_3x_3$ we have an inactive neuron as long as the weighted sum is not greater than $+20$\n",
    "* To standardize the description, we will systematically put $x_0=+1$ for the input data linked to $b$ (his weight)\n",
    "* We sometimes see the convention $b =w_0x_{0}$ to have a simpler writing of $z$ => $z = b + \\sum_{k=1}^{N}w_kx_{k}$ or $z = \\sum_{k=0}^{N}w_kx_{k}$\n",
    "\n",
    "\n",
    "**Activation function** :  $\\sigma$ is called the activation function. Here we will use the **sigmoid** function which gives a nonlinear distribution between $0$ and $1$. But this function may be different depending on what we want to understand https://en.wikipedia.org/wiki/Activation_function\n",
    "\n",
    "![image](./images/deep-ML-sigmoid.png)\n",
    "\n",
    "\n",
    "**Function Derivate of** $\\sigma(z)$ :\n",
    "\n",
    "$$\\sigma '(z)=\\frac{e^{-z}}{(1+e^{-z})^{2}} = \\frac{1}{1+e^{-z}}(1-\\frac{1}{1+e^{-z}})=\\sigma(z)(1-\\sigma(z))  \\qquad \\qquad (1.2)$$\n",
    "\n",
    "\n",
    "### Neural Network\n",
    "\n",
    "We build a multi-layer network by connecting \"Layers\" of neurons to each other. So the input of one neuron can be the output of another\n",
    "\n",
    "![image](./images/deep-ML-Note-02.png)\n",
    "\n",
    "$4$ Layers:\n",
    "* Input : First Layer. It takes the data and passes it to the next Layer without operation\n",
    "* Hidden : They include the layers of neurons that are not seen in the \"Training Set\". They apply operations on the data received as inputs\n",
    "* Output : It receives the data from the last Layer as input and produces the output data\n",
    "\n",
    "We introduce here the following notations :\n",
    "* $L$ Total number of Layers\n",
    "* $s_{l}$ Number of Nodes (excepted bias) of Layer $l$\n",
    "* $x \\in \\mathbb{R}^{N}$ input vector, $y\\in \\mathbb{R}^{K}$ output vector\n",
    "* $W^{(l)}$ weight matrix connecting the Layers $l-1$ and $l$\n",
    "* $w^{(l)}_{jk}$ weight connecting node $k$ of Layer $l-1$ and Node $j$ of Layer $l$ *(The row index of the matrix therefore represents the index of the target node of the layer $l$)*\n",
    "* $b^{(l)}$ bias vector connecting Layers $l-1$ and $l$\n",
    "* $z^{(l)}_{j} = b^{(l)}_{j} + \\sum_{k=1}^{s_{l-1}}w^{(l)}_{jk}a^{(l-1)}_{k}$\n",
    "* $a^{(l)}$ activation vector of Layer $l$\n",
    "* $a^{(l)}_{j}$ the value calculated by the activation function of the unit $j$ in the Layer $l$\n",
    "$$a^{(l)}_{j}=\\sigma(b^{(l)}_{j} + \\sum_{k=1}^{s_{l-1}}w^{(l)}_{jk}a^{(l-1)}_{k})$$\n",
    "* $\\sigma$ activation function. Here **sigmoid** function\n",
    "\n",
    "For the network represented in *figure 2* we have :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}          \n",
    "&W^{(1 and 2)}\\in \\mathbb{R}^{3\\times 3} \\\\\n",
    "&W^{(3)}\\in \\mathbb{R}^{1\\times 3} \\\\\n",
    "&b^{(1 and 2)}\\in \\mathbb{R}^{1\\times 3} \\\\\n",
    "&b^{(3)}\\in \\mathbb{R}^{1\\times 1} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}          \n",
    "&a^{(1)}_{1}=\\sigma(b^{(1)}_{1} + w^{(1)}_{11}x_1 + w^{(1)}_{12}x_2 + w^{(1)}_{13}x_3) \\\\\n",
    "&a^{(1)}_{2}=\\sigma(b^{(1)}_{2} + w^{(1)}_{21}x_1 + w^{(1)}_{22}x_2 + w^{(1)}_{23}x_3) \\\\\n",
    "&a^{(1)}_{3}=\\sigma(b^{(1)}_{3} + w^{(1)}_{31}x_1 + w^{(1)}_{32}x_2 + w^{(1)}_{33}x_3)\\\\\n",
    "\\\\\n",
    "&a^{(2)}_{1}=\\sigma(b^{(2)}_{1} + w^{(2)}_{11}a^{(1)}_{1} + w^{(2)}_{12}a^{(1)}_{2} + w^{(2)}_{13}a^{(1)}_{3}) \\\\\n",
    "&a^{(2)}_{2}=\\sigma(b^{(2)}_{2} + w^{(2)}_{21}a^{(1)}_{1} + w^{(2)}_{22}a^{(1)}_{2} + w^{(2)}_{23}a^{(1)}_{3}) \\\\\n",
    "&a^{(2)}_{3}=\\sigma(b^{(2)}_{3} + w^{(2)}_{31}a^{(1)}_{1} + w^{(2)}_{32}a^{(1)}_{2} + w^{(2)}_{33}a^{(1)}_{3}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$h_{\\mathbf{w}}(x)=a^{(3)}_{1}=\\sigma(b^{(3)}_{1} + w^{(3)}_{11}a^{(2)}_{1} + w^{(3)}_{12}a^{(2)}_{2} + w^{(3)}_{13}a^{(2)}_{3})$$\n",
    "\n",
    "Using the previous standard notation, we will note :\n",
    "$$a^{(j)}_{i}=\\sigma(z^{(j)}_{i})$$\n",
    "$$z^{(j)}=\\mathbf{w}^{(j-1)}.\\mathbf{x}$$\n",
    "$$a^{(j)}=\\sigma(z^{(j)})$$\n",
    "\n",
    "\n",
    "The Output Layer not necessary contains one unit. It can have several as below\n",
    "\n",
    "![image](./images/deep-ML-networkMultiOutput.png)\n",
    "\n",
    "\n",
    "Here $h_{\\mathbf{w}}(x) \\in \\mathbb{R}^{3}$\n",
    "\n",
    "For example, we can build the network to have outputs of type\n",
    "\n",
    "$h_{\\mathbf{w}}(x)\\approx\\left(\\begin{matrix} 1 \\\\ 0 \\\\ 0 \\end{matrix} \\right)$ => a Cat,\n",
    "$h_{\\mathbf{w}}(x)\\approx\\left(\\begin{matrix} 0 \\\\ 1 \\\\ 0 \\end{matrix} \\right)$ => a Dog,\n",
    "$h_{\\mathbf{w}}(x)\\approx\\left(\\begin{matrix} 0 \\\\ 0 \\\\ 1 \\end{matrix} \\right)$ => a Bird\n",
    "\n",
    "\n",
    "### Activation function\n",
    "\n",
    "The activation function $\\sigma$ is important because it allows to introduce nonlinearity into the neural network. It will make it possible to modelize a nonlinear problem.\n",
    "\n",
    "Moreover it is important for the Gradient Descent and the calculations of derivatives of **backpropagation** algorithm - explained in the continuation - that this function is continuous and does not include jumps.\n",
    "\n",
    "### Example : The \"XNOR\"\n",
    "\n",
    "\n",
    "![image](./images/deep-ML-XNOR.png)\n",
    "\n",
    "\n",
    "We have here a network with 3 inputs and 2 Layers whose objective is to implement a ***[XNOR] (https://en.wikipedia.org/wiki/XNOR_gate)*** logic. The weights indicated in the diagram are fixed for illustration but a calculation by learning method is proposed below.\n",
    "\n",
    "We have : \n",
    "$$\n",
    "\\begin{aligned}          \n",
    "&x_1,x_2 \\in \\{0,1\\} \\\\\n",
    "\\\\\n",
    "&a^{(1)}_{1}=\\sigma(-30+20x_1+20x_2) \\\\\n",
    "&a^{(1)}_{2}=\\sigma(10-20x_1-20x_2) \\\\\n",
    "&a^{(2)}_{1}=\\sigma(-10+20a^{(1)}_{1}+20a^{(1)}_{2}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "| $x_1$ | $x_2$ |  $a^{(1)}_{1}$ | $a^{(1)}_{2}$ |  h(x)   |\n",
    "|:------:|:-----:|:--------:|:--------:|:--------:|\n",
    "|   &nbsp;0&nbsp;   |    &nbsp;0&nbsp;    |   &nbsp;0&nbsp;   |  &nbsp;1&nbsp;   |        $\\approx$ 1         |\n",
    "|  1  |  0  | 0  | 0  |  $\\approx$ 0         |\n",
    "|  0  |  1  | 0  | 0  |  $\\approx$ 0         |\n",
    "|  1  |  1  | 1 | 0  |  $\\approx$ 1         |\n",
    "\n",
    "=> The Network with its weights and bias well simulate ***XNOR***\n",
    "\n",
    "\n",
    "### Forward propagation\n",
    "\n",
    "Function $h_{\\mathbf{w},\\mathbf{b}}(x)$ representing the Neural Network is a composed function which is expressed in matrix form :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}          \n",
    "\\mathbf{a}^{(0)}&= \\mathbf{x}\\\\\n",
    "\\mathbf{z}^{(1)}&= \\mathbf{W}^{(1)}.\\mathbf{a}^{(0)}+\\mathbf{b}^{(1)} \\\\\n",
    "\\mathbf{a}^{(1)} &= \\sigma(\\mathbf{z}^{(1)}) \\\\\n",
    "\\mathbf{z}^{(2)}&= \\mathbf{W}^{(2)}.\\mathbf{a}^{(1)}+\\mathbf{b}^{(2)} \\\\\n",
    "\\mathbf{a}^{(2)} &= \\sigma(\\mathbf{z}^{(2)}) \\\\\n",
    "&..... \\\\\n",
    "\\mathbf{a}^{(L)} &= h_{\\mathbf{w},\\mathbf{b}}(x) = \\sigma(\\mathbf{z}^{(L)})=Y \\\\       \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "## The Cost Function\n",
    "\n",
    "\n",
    "The objective of the training is to calculate the weights as well as possible to obtain a minimum error between the expected data and the data calculated by the neural network. For this we pass a training dataset $\\{(x^{(1)},y^{(1)}),...,(x^{(m)},y^{(m)})\\}$ in the neural network and we minimize a Cost function $E$ representing a measure of the total error between the calculated outputs and the expected ones\n",
    "\n",
    "We have :\n",
    "* $L$ Total nuber of Layers\n",
    "* $x \\in \\mathbb{R}^{N}$ we have the norm $\\left\\|x\\right\\|_{2} = (\\sum\\limits_{i=1}^{N}{x^{2}_{i}} )^{\\frac{1}{2} }$\n",
    "* Training Set : $\\{(x^{(1)},y^{(1)}),...,(x^{(m)},y^{(m)})\\}$\n",
    "* $E$ the Cost Function to minimize\n",
    "* $h_{\\mathbf{w},\\mathbf{b}}(x)$ is the function representing the network and producing the output data\n",
    "\n",
    "We define $E$ as the sum of the errors on all the training data by :\n",
    "\n",
    "$$E(\\mathbf{w},\\mathbf{b}) = \\frac{1}{m}\\sum\\limits_{t=1}^{m}E(\\mathbf{w},\\mathbf{b})_{t} \\qquad \\qquad (COST)$$\n",
    "\n",
    "with \n",
    "\n",
    "$$E(\\mathbf{w},\\mathbf{b})_{t}=\\frac{1}{2}\\left\\|y^{(t)}-h_{\\mathbf{w},\\mathbf{b}}(x^{(t)})\\right\\|^{2}_{2} \\qquad \\qquad (COSTt)$$\n",
    "\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Minimizing $E$ is therefore minimizing the error for each training data $(COSTt)$ then taking the average for all the data\n",
    "\n",
    "We can see that $(COSTt)$ depends on weights $w^{(l)}_{jk}$, bias $b^{(l)}_{j}$ and values of activation functions $a^{(l)}_{j}$ calculated for each Node. But we can only \"play\" with the weights and the bias\n",
    "\n",
    "*This cost function can be used for both classification and regression problem. For Classification $y \\in \\{0,1\\}$ the output is calculated by the Sigmoid function giving a distributuon in $[0,1]$. For a regression problem $y \\in [0,1]$*\n",
    "\n",
    "\n",
    "The Cost Function $(COST)$ can be minimized (regarding $\\mathbf{w}$ and $\\mathbf{b}$) by the Gradient Descent Method [Gradient Descent](./ml-introduction-suplearn-reg.ipynb#Gradient-Descent).\n",
    "\n",
    "Minimizing is to \"get down\" in the opposite direction of the gradient which gives for each point the direction and the value of the greatest slope.\n",
    "\n",
    "The figure below illustrates a Cost Function $E$ depending on 2 weights $w_{1}$ and $w_{2}$\n",
    "* On the left the Cost function\n",
    "* On the right a projection of the level lines and the gradient fields at each point. The gradient at a point is perpendicular to the level line\n",
    "\n",
    "![image](./images/deep-ML-CostPlot.png)\n",
    "\n",
    "But unlike what is done here [Gradient Descent](./ml-introduction-suplearn-reg.ipynb#Gradient-Descent) there are Hidden Layers whose output and input values are not known. We only have the $\\{(x^{(i)},y^{(i)})\\}$ of the first and last Layer. We have to find the minimum of $(COST)$ regarding the weights $w^{(l)}_{jk}$ and $b^{(l)}_{j}$ going downhill \"***in the fog***\". We cannot modify the activation values, we can only play on the weights and bias !!!\n",
    "\n",
    "We will therefore make a descent by calculating the gradients of $E(\\mathbf{w},\\mathbf{b})_{t}$ relativaly to $w$ and $b$ for each Layer $l$. Derivative calculations will allow us to see the influence that a disturbance of a given weight/bias has on the error, and therefore to be able to adjust. This is the **backpropagation** algoritm one of the creators is **Yann Le Cun**\n",
    "\n",
    "$s_{l}$ being the Layer $l$ number of Nodes (without bias)\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\Large\n",
    "\\nabla_{\\mathbf{w^{(l)}}}E_{t} = \\left(\\begin{matrix} \n",
    "\\frac{\\partial E_{t}}{\\partial w^{(l)}_{11}} & \\frac{\\partial E_{t}}{\\partial w^{(l)}_{12}} & ... & \\frac{\\partial E_{t}}{\\partial w^{(l)}_{1s_{l-1}}} \\\\ \n",
    "\\frac{\\partial E_{t}}{\\partial w^{(l)}_{21}} & \\frac{\\partial E_{t}}{\\partial w^{(l)}_{22}} & ... & \\frac{\\partial E_{t}}{\\partial w^{(l)}_{2s_{l-1}}} \\\\ \n",
    "... & ... & .... \\\\\n",
    "\\frac{\\partial E_{t}}{\\partial w^{(l)}_{s_{l}1}} & \\frac{\\partial E_{t}}{\\partial w^{(l)}_{s_{l}2}} & ... & \\frac{\\partial E_{t}}{\\partial w^{(l)}_{s_{l}s_{l-1}}} \\\\ \n",
    "\\end{matrix} \\right) \\qquad \\qquad\n",
    "\\nabla_{\\mathbf{b^{(l)}}}E_{t} = \\left(\\begin{matrix} \n",
    "\\frac{\\partial E_{t}}{\\partial b^{(l)}_{1}} \\\\\n",
    "\\frac{\\partial E_{t}}{\\partial b^{(l)}_{2}} \\\\\n",
    "... \\\\\n",
    "\\frac{\\partial E_{t}}{\\partial b^{(l)}_{s_{l}}} \\\\\n",
    "\\end{matrix} \\right)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "An image character recognition neuron network can have millions of links. In that case calculating $(COSTt)$ on all training data and averaging can be very expensive. To avoid this we randomly take a subset of the training data set, then we calculate the gradients and so on. We decrease the descent step $\\alpha$ as we approach the \"valley bottom\".\n",
    "\n",
    "Thus appear the following notions :\n",
    "* **Batch** : The Training Set\n",
    "* **Mini-Batch** : Subset of Training Set\n",
    "* **Epochs** : Number of **Mini-Batch** executed\n",
    "\n",
    "> In the following, we explain the **Backpropagation** algorithm by taking **Mini-Batch** = set of training data.\n",
    "\n",
    "https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3\n",
    "\n",
    "### What about $E$ convexity ?\n",
    "\n",
    "The gradient descent algorithm achieves the global minimum of the cost function $E$ only if it is convex. In the case of neural networks, this function is generally not convex and therefore comprises many \"valleys\" or minima. $E$ is convex if its Hessian matrix (second derivative) is positive-definite. \n",
    "\n",
    "We will not do the calculation here but we can see it simply by noticing that the cost function is not modified if we exchange the weights $w^{l}_{jk}$ and $w^{l}_{jm}$ arriving at neuron $a^{l}_{j}$. We therefore have 2 different neural networks and we have found 2 local minima of $E$.\n",
    "\n",
    "![image](./images/deep-ML-CostConvexe.png)\n",
    "\n",
    "\n",
    "Despite this fact which can appear as problematic we will succeed by the method described below to build a correct network model. There is no formal explanation for this. Neural Networks work better than other methods but we do not know how to fully explain it. *It is perhaps one of the doors allowing the production of a \"magic\" speech on AI*\n",
    "\n",
    "\n",
    "## Backpropagation Algorithm\n",
    "\n",
    "To do this we use an approach known as ***Backpropagation***. It will consist in determining the influence of the different variables $z^{(l)}_{j}, a^{(l)}_{j}, b^{(l)}_{j}, w^{(l)}_{jk}$ on Cost Function $(COSTt)$ by calculating their derivatives.\n",
    "\n",
    "Algorithm main steps :\n",
    "1. Randomly fix $w^{(l)}_{jk}$ and $b^{(l)}_{j}$ for $l \\in {1,2,...,L}$\n",
    "2. **Repeat until $epochs$** (nb iterations) **or convergence condition on $E$**\n",
    "2. **For each** $ x=(x^{(t)},y^{(t)}) \\in TrainingSet$\n",
    "    * Calculate $z^{(l),t}_{j}$ and $a^{(l),t}_{j}$ on all Layers $1,2,...,L$=> **Forward propagation**\n",
    "    * Calculate $E_{t}$ and first derivatives of the Output Layer $L$ \n",
    "    $$\\frac{\\partial E_{t}}{\\partial w^{(L)}_{jk}}$$ and $$\\frac{\\partial E_{t}}{\\partial b^{(L)}_{j}}$$\n",
    "<br>\n",
    "    * **backpropagate** in order to calculate the derivatives for all the Layers $(L-1),(L-2), ... , 2$ \n",
    "    \n",
    "    $$\\frac{\\partial E_{t}}{\\partial w^{(l)}_{jk}}$$ and $$\\frac{\\partial E_{t}}{\\partial b^{(l)}_{j}}$$ \n",
    "    \n",
    "6. **Gradient Descent** *Update the weights and bias by averaging*\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w^{(l)}_{jk} &= w^{(l)}_{jk} - \\frac{\\alpha}{m} \\sum\\limits_{t=1}^{m}{\\frac{\\partial E_{t}}{\\partial w^{(l)}_{jk}}}\\\\\n",
    "b^{(l)}_{j} &= b^{(l)}_{j} - \\frac{\\alpha}{m} \\sum\\limits_{t=1}^{m}{\\frac{\\partial E_{t}}{\\partial b^{(l)}_{j}}}\\\\\n",
    "&\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "The figure below represents the situation we have to understand at the level of derivatives calculation\n",
    "\n",
    "![image](./images/deep-ML-networkBackpropagation.png)\n",
    "\n",
    "> To simplify notations, we will omit the index $t$ referencing the training data $z^{(l),t}_{j}$ and $a^{(l),t}_{j}$ writing $z^{(l)}_{j}$ et $a^{(l)}_{j}$\n",
    "\n",
    "\n",
    "### Chain Rule\n",
    "\n",
    "The [Chain Rule](https://en.wikipedia.org/wiki/Chain_rule) on derivatives is the mathematical key point of **backpropagation** algorithm. It is used to calculate the influence of a variable on the output error.\n",
    "\n",
    "It is based on derivatives of composite functions :\n",
    "\n",
    "$$f(g(x))'=f'(g(x)).g'(x)$$ \n",
    "putting $g(x)=y$ and $f(y)=z$ we have\n",
    "$$\\frac{dz}{dx} = \\frac{dz}{dy}.\\frac{dy}{dx} $$\n",
    "\n",
    "We will apply it to $(COSTt)$\n",
    "\n",
    "\n",
    "### Calculation of $\\frac{\\partial E_{t}}{\\partial w^{(l)}_{jk}}$\n",
    "\n",
    "We use here all the notations defined above + reminder :\n",
    "\n",
    "$$z^{(l)}_{j} = b^{(l)}_{j} + \\sum_{k}w^{(l)}_{jk}a^{(l-1)}_{k}$$\n",
    "$$a^{(l)}_{j} = \\sigma(z^{(l)}_{j})$$\n",
    "\n",
    "Matrix form :\n",
    "$$\\mathbf{z}^{(l)} = \\mathbf{b}^{(l-1)} + \\mathbf{w}^{(l)}\\mathbf{a}^{(l-1)}$$\n",
    "$$\\mathbf{a}^{(l)} = \\sigma(\\mathbf{z}^{(l)})$$\n",
    "\n",
    "The calculations are done with the training data fixed. So we have\n",
    "\n",
    "$$E(\\mathbf{w},\\mathbf{b})_{t}=\\frac{1}{2}\\left\\|y^{(t)}-a^{L})\\right\\|^{2}_{2}$$\n",
    "\n",
    "The derivative $\\frac{\\partial E_{t}}{\\partial w^{(l)}_{jk}}$ evaluates the variation impact of $w^{(l)}_{jk}$ on final error $E_{t}$. The preceding figure shows that this variation impact $z^{(l)}_{j}$ which impacts $a^{(l)}_{j}$ which influence $E_{t}$. By applying the **chain rule** we see that the derivative therefore integrates 3 components.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_{t}}{\\partial w^{(l)}_{jk}}= \\frac{\\partial E_{t}}{\\partial a^{(l)}_{j}}\\frac{\\partial a^{(l)}_{j}}{\\partial w^{(l)}_{jk}}=\\frac{\\partial E_{t}}{\\partial a^{(l)}_{j}}\\frac{\\partial a^{(l)}_{j}}{\\partial z^{(l)}_{j}}\\frac{\\partial z^{(l)}_{j}}{\\partial w^{(l)}_{jk}} \\qquad \\qquad (3.1)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Case $l=L$ Output Layer**\n",
    "\n",
    "$$z^{(L)}_{j} = b^{(L)}_{j} + \\sum_{k}w^{(L)}_{jk}a^{(L-1)}_{k}$$\n",
    "\n",
    "We can write $(COSTt)$ as\n",
    "\n",
    "$$E(\\mathbf{w},\\mathbf{b})_{t}=\\frac{1}{2}\\left\\|y^{(t)}-a^{L})\\right\\|^{2}_{2}$$\n",
    "\n",
    "We simply get by derivation\n",
    "\n",
    "$$\n",
    "\\begin{aligned}          \n",
    "&\\frac{\\partial E_{t}}{\\partial a^{(L)}_{j}}=(a^{(L)}_{j}-y^{(t)}) \\\\\n",
    "&\\frac{\\partial z^{(L)}_{j}}{\\partial w^{(L)}_{jk}}=a^{(L-1)}_{k} \\\\\n",
    "&\\frac{\\partial a^{(L)}_{j}}{\\partial z^{(L)}_{j}}=\\sigma '(z^{(L)}_{j}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "according to $(1.2)$\n",
    "\n",
    "$$\\sigma'(z)=\\sigma(z)(1-\\sigma(z))$$\n",
    "\n",
    "so $$\\frac{\\partial a^{(L)}_{j}}{\\partial z^{(L)}_{j}}=a^{(L)}_{j}(1-a^{(L)}_{j})$$\n",
    "\n",
    "\n",
    "$(3.1)$ is therefore written\n",
    "\n",
    "$$\\frac{\\partial E_{t}}{\\partial w^{(L)}_{jk}}=(a^{(L)}_{j}-y^{(t)})a^{(L)}_{j}(1-a^{(L)}_{j})a^{(L-1)}_{k}  \\qquad \\qquad (3.2)$$\n",
    "\n",
    "\n",
    "**General Case**\n",
    "\n",
    "We have as before\n",
    "\n",
    "$$\n",
    "\\begin{aligned}          \n",
    "&\\frac{\\partial a^{(l)}_{j}}{\\partial z^{(l)}_{j}}=\\sigma '(z^{(l)}_{j}) \\\\\n",
    "&\\frac{\\partial z^{(l)}_{j}}{\\partial w^{(l)}_{jk}}=a^{(l-1)}_{k} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "But here the derivative $\\frac{\\partial E_{t}}{\\partial a^{(l)}_{j}}$ of $(3.1)$ is more complex. As we can see in the previous diagram we must evaluate how the variation of $a^{(l)}_{j}$ influence the following Nodes. Here the variation acts on all the nodes $a^{(l+1)}_{j}$ of the following Layer. By applying the **chain rule** we get\n",
    "\n",
    "$$\\frac{\\partial E_{t}}{\\partial a^{(l)}_{j}} = \\sum\\limits_{i=1}^{s_{l+1}}{\\frac{\\partial E_{t}}{\\partial a^{(l+1)}_{i}}\\frac{\\partial a^{(l+1)}_{i}}{\\partial z^{(l+1)}_{i}}\\frac{\\partial z^{(l+1)}_{i}}{\\partial a^{(l)}_{j}} }$$\n",
    "\n",
    "\n",
    "$(3.1)$ is therefore written\n",
    "\n",
    "$$\\frac{\\partial E_{t}}{\\partial w^{(l)}_{jk}} = \\frac{\\partial E_{t}}{\\partial a^{(l)}_{j}}\\sigma '(z^{(l)}_{j})a^{(l-1)}_{k}$$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial E_{t}}{\\partial w^{(l)}_{jk}} = \\sigma '(z^{(l)}_{j})a^{(l-1)}_{k}\\sum\\limits_{i=1}^{s_{l+1}}{\\frac{\\partial E_{t}}{\\partial a^{(l+1)}_{i}}\\frac{\\partial a^{(l+1)}_{i}}{\\partial z^{(l+1)}_{i}}\\frac{\\partial z^{(l+1)}_{i}}{\\partial a^{(l)}_{j}} }$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\\begin{aligned}          \n",
    "&\\frac{\\partial a^{(l+1)}_{i}}{\\partial z^{(l+1)}_{i}}=\\sigma '(z^{(l+1)}_{i}) \\\\\n",
    "&\\frac{\\partial z^{(l+1)}_{i}}{\\partial a^{(l)}_{j}}=w^{(l+1)}_{ij} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\\frac{\\partial E_{t}}{\\partial w^{(l)}_{jk}} = \\sigma '(z^{(l)}_{j})a^{(l-1)}_{k}\\sum\\limits_{i=1}^{s_{l+1}}{\\frac{\\partial E_{t}}{\\partial a^{(l+1)}_{i}} \\sigma '(z^{(l+1)}_{i})w^{(l+1)}_{ij}}  \\qquad \\qquad (3.3)$$\n",
    "\n",
    "**The calculated values** : \n",
    "* We know all terms $z^{(..)}_{j}, a^{(..)}_{j}$ calculated during propagation\n",
    "* We know all weights $w^{(..)}_{ij}$\n",
    "* And we know $\\frac{\\partial E_{t}}{\\partial a^{(l+1)}_{i}}$ calculated previously => **Here is the Key Point !!!**\n",
    "\n",
    "\n",
    "### Calculation of $\\frac{\\partial E_{t}}{\\partial b^{(l)}_{j}}$\n",
    "\n",
    "We have $(3.1)$ equivalent by replacing the weight by the bias\n",
    "\n",
    "$$\\frac{\\partial E_{t}}{\\partial b^{(l)}_{j}} =  \\frac{\\partial E_{t}}{\\partial a^{(l)}_{j}}\\frac{\\partial a^{(l)}_{j}}{\\partial b^{(l)}_{j}}=\\frac{\\partial E_{t}}{\\partial a^{(l)}_{j}}\\frac{\\partial a^{(l)}_{j}}{\\partial z^{(l)}_{j}}\\frac{\\partial z^{(l)}_{j}}{\\partial b^{(l)}_{j}}   \\qquad \\qquad (3.4)$$\n",
    "\n",
    "**Case $l=L$ Output Layer**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}          \n",
    "&\\frac{\\partial E_{t}}{\\partial a^{(L)}_{j}}=-(y^{(t)}-a^{(L)}_{j}) \\\\\n",
    "&\\frac{\\partial a^{(L)}_{j}}{\\partial z^{(L)}_{j}}=a^{(L)}_{j}(1-a^{(L)}_{j}) \\\\\n",
    "&\\frac{\\partial z^{(L)}_{j}}{\\partial b^{(L)}_{j}} = 1 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$(3.1)$ is written :\n",
    "\n",
    "$$\\frac{\\partial E_{t}}{\\partial b^{(L)}_{j}} =-(y^{(t)}-a^{(L)}_{j})a^{(L)}_{j}(1-a^{(L)}_{j})    \\qquad \\qquad (3.5)$$\n",
    "\n",
    "**General Case**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}          \n",
    "&\\frac{\\partial a^{(l)}_{j}}{\\partial z^{(l)}_{j}}=\\sigma '(z^{(l)}_{j}) \\\\\n",
    "&\\frac{\\partial z^{(l)}_{j}}{\\partial b^{(l)}_{j}}=1 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial E_{t}}{\\partial b^{(l)}_{j}} = \\sigma '(z^{(l)}_{j})\\sum\\limits_{i=0}^{s_{l+1}}{\\frac{\\partial E_{t}}{\\partial a^{(l+1)}_{i}}\\frac{\\partial a^{(l+1)}_{i}}{\\partial z^{(l+1)}_{i}}\\frac{\\partial z^{(l+1)}_{i}}{\\partial a^{(l)}_{j}} }$$\n",
    "\n",
    "thus\n",
    "\n",
    "$$\\frac{\\partial E_{t}}{\\partial b^{(l)}_{j}} = \\sigma '(z^{(l)}_{j})\\sum\\limits_{i=0}^{s_{l+1}}{\\frac{\\partial E_{t}}{\\partial a^{(l+1)}_{i}} \\sigma '(z^{(l+1)}_{i})w^{(l+1)}_{ij}}  \\qquad \\qquad (3.6)$$\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "### Neural Network implementing XOR\n",
    "\n",
    "The Objective is to implement an exclusive OR neural network whose truth table is :\n",
    "\n",
    "| $A$ | $B$ | XOR  | \n",
    "|:------:|:-----:|:--------:|\n",
    "|  &nbsp;0&nbsp;  |  &nbsp;0&nbsp;  | &nbsp;&nbsp;0&nbsp;&nbsp;  |\n",
    "|  1  |  0  | 1  |\n",
    "|  0  |  1  | 1  | \n",
    "|  1  |  1  | 0 | \n",
    "\n",
    "It is a classification problem whose decision bound is not linear. We cannot separate the points by a straight line. The question we have to ask ourselves is how should I design the network to implement the function ?\n",
    "\n",
    "![image](./images/deep-ML-Example-XOR-boundary.png)\n",
    "\n",
    "If we take the simplest network called **Sigmoid Perceptron**\n",
    "\n",
    "![image](./images/deep-ML-Example-XOR-Perceptron.png)\n",
    "\n",
    "we have $$h_{\\mathbf{w}}(x)=\\sigma(w^{(1)}_{11}x_{1}+w^{(1)}_{12}x_{2}+b^{(1)}_{1})$$\n",
    "\n",
    "In spite the nonlinearity of activation function (sigmoid) this network can simulate correctly a single decision boundary.\n",
    "\n",
    "Knowing that we must have 2 separation boundaries, we build a Hidden Layer made up of 2 neurons which each will position a border. A detailed explanation and truth tables for each neuron can be found here http://toritris.weebly.com/perceptron-5-xor-how--why-neurons-work-together.html. The second layeur will do the \"merge\" and is made up of 1 neuron.\n",
    "\n",
    "![image](./images/deep-ML-Example-XOR.png)\n",
    "\n",
    "> Note: We could create a different neural network, for example by introducing a second intermediate layer, and it would converge towards the same result. The backpropagation algorithm would balance the weights and bias minimizing the cost and the network output would \"stick\" to the inputs !!!\n",
    "\n",
    "The python script below implements **the backpropagation algorithm** step by step. We deliberately made the calculations appear without using a compact instruction of the vector product type.\n",
    "\n",
    "At the output we can see that the values predicted by the neural network correspond to the expected truth table and that the error $E$ has been minimized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Hidden weights W1= [[3.82599745 3.8238028 ]\n",
      " [5.86209415 5.85137987]]\n",
      "Final Hidden bias B1= [-5.85460228 -2.44865491]\n",
      "Final Hidden weights W2= [[-8.28509454  7.68244591]]\n",
      "Final Hidden bias B2= -3.486433762700182\n",
      "\n",
      "Output calculated after  30000  epochs:  [0.05219916 0.95200298 0.95203435 0.05167569]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "# Intiatilization of weights and Bias for the XOR function (1 Hidden layer)\n",
    "W1 = np.random.uniform(size=(2,2))\n",
    "W2 = np.random.uniform(size=(1,2))\n",
    "B1 = np.random.uniform(size=(2)).T\n",
    "B2 = 0.1\n",
    "\n",
    "# Batch : Training examples\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([0,1,1,0]).T\n",
    "\n",
    "#Activation neurones\n",
    "Z1 = np.zeros(2)\n",
    "Z2 = np.zeros(1)\n",
    "A1 = np.zeros(2)\n",
    "A2 = np.zeros(1)\n",
    "\n",
    "alpha = 0.15\n",
    "m = 4 #number of trainning\n",
    "epochs = 30000\n",
    "\n",
    "costs = np.zeros(epochs)\n",
    "\n",
    "output_calculated = np.zeros(4)\n",
    "\n",
    "for j in range(epochs):    \n",
    "    cost_j = 0\n",
    "    sum_dW1 = np.zeros((2,2))\n",
    "    sum_dW2 = np.zeros((1,2))\n",
    "    sum_dB1 = np.zeros((2,1))\n",
    "    sum_dB2 = 0    \n",
    "    for i in range(X.shape[0]):\n",
    "        #Forward Propagation \n",
    "        Z1[0] = np.dot(W1[0,:],X[i]) + B1[0]\n",
    "        Z1[1] = np.dot(W1[1,:],X[i]) + B1[1]\n",
    "        A1[0] = sigmoid(Z1[0])\n",
    "        A1[1] = sigmoid(Z1[1])\n",
    "        Z2[0] = np.dot(W2[0,:],A1) + B2        \n",
    "        A2[0] = sigmoid(Z2[0])\n",
    "\n",
    "        output_calculated[i] = A2[0]\n",
    "\n",
    "        # Error        \n",
    "        cost_j = cost_j + 1/2*(A2[0] - y[i])**2\n",
    "        #--------------Back Propagation--------------\n",
    "        dW1 = np.zeros((2,2))\n",
    "        dW2 = np.zeros((1,2))\n",
    "        dB1 = np.zeros((2,1))\n",
    "        dB2 = 0        \n",
    "        #Output Layer\n",
    "        grad_pred = (A2[0] - y[i])       \n",
    "        dW2[0][0] =  grad_pred*A2[0]*(1 - A2[0])*A1[0]\n",
    "        dW2[0][1] =  grad_pred*A2[0]*(1 - A2[0])*A1[1]\n",
    "        dB2 = grad_pred*A2[0]*(1 - A2[0])\n",
    "         \n",
    "        #Hidden Layer\n",
    "        dW1[0][0] = grad_pred*sigmoid_prime(A1[0])*X[i][0]*sigmoid_prime(A2[0])*W2[0][0]\n",
    "        dW1[0][1] = grad_pred*sigmoid_prime(A1[0])*X[i][1]*sigmoid_prime(A2[0])*W2[0][0]\n",
    "        dW1[1][0] = grad_pred*sigmoid_prime(A1[1])*X[i][0]*sigmoid_prime(A2[0])*W2[0][1]\n",
    "        dW1[1][1] = grad_pred*sigmoid_prime(A1[1])*X[i][1]*sigmoid_prime(A2[0])*W2[0][1]\n",
    "        dB1[0] = grad_pred*sigmoid_prime(A1[0])*sigmoid_prime(A2[0])*W2[0][0]\n",
    "        dB1[1] = grad_pred*sigmoid_prime(A1[1])*sigmoid_prime(A2[0])*W2[0][1]\n",
    "\n",
    "        #Sum Weights and Bias derivative for AVG\n",
    "        sum_dW2[0][0] = sum_dW2[0][0] + dW2[0][0]\n",
    "        sum_dW2[0][1] = sum_dW2[0][1] + dW2[0][1]\n",
    "        sum_dB2 = sum_dB2 + dB2\n",
    "        sum_dW1[0][0] = sum_dW1[0][0] + dW1[0][0]\n",
    "        sum_dW1[0][1] = sum_dW1[0][1] + dW1[0][1]\n",
    "        sum_dW1[1][0] = sum_dW1[1][0] + dW1[1][0]\n",
    "        sum_dW1[1][1] = sum_dW1[1][1] + dW1[1][1]\n",
    "        sum_dB1[0] = sum_dB1[0] + dB1[0]\n",
    "        sum_dB1[1] = sum_dB1[1] + dB1[1]\n",
    "\n",
    "    costs[j] = cost_j\n",
    "\n",
    "    if cost_j < 0.005:\n",
    "        break\n",
    "    #update weights and bias\n",
    "    W1[0][0] = W1[0][0] - (alpha/m)*sum_dW1[0][0]\n",
    "    W1[0][1] = W1[0][1] - (alpha/m)*sum_dW1[0][1]\n",
    "    W1[1][0] = W1[1][0] - (alpha/m)*sum_dW1[1][0]\n",
    "    W1[1][1] = W1[1][1] - (alpha/m)*sum_dW1[1][1]\n",
    "    W2[0][0] = W2[0][0] - (alpha/m)*sum_dW2[0][0]\n",
    "    W2[0][1] = W2[0][1] - (alpha/m)*sum_dW2[0][1]\n",
    "    B1[0] = B1[0] - (alpha/m)*sum_dB1[0] \n",
    "    B1[1] = B1[1] - (alpha/m)*sum_dB1[1]\n",
    "    B2 = B2 - (alpha/m)*sum_dB2\n",
    "    \n",
    "\n",
    "print('Final Hidden weights W1=',W1)\n",
    "print('Final Hidden bias B1=',B1)\n",
    "print('Final Hidden weights W2=',W2)\n",
    "print('Final Hidden bias B2=',B2)\n",
    "print(\"\\nOutput calculated after \",epochs,\" epochs: \",output_calculated)\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.plot(np.arange(0,epochs), costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
